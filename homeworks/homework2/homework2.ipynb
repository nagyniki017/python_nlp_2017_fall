{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Your name]\n",
    "\n",
    "\n",
    "# Homework 2\n",
    "\n",
    "The maximum score of this homework is 100 points. Grading is listed in this table:\n",
    "\n",
    "| Grade | Score range |\n",
    "| --- | --- |\n",
    "| 5 | 85+ |\n",
    "| 4 | 70-84 |\n",
    "| 3 | 55-69 |\n",
    "| 2 | 40-54 |\n",
    "| 1 | 0-39 |\n",
    "\n",
    "Most exercises include tests which should pass if your solution is correct.\n",
    "However successful test do not guarantee that your solution is correct.\n",
    "You are free to add more tests.\n",
    "\n",
    "## Deadline\n",
    "__2017 November 20<sup>th</sup> Monday 23:59__\n",
    "\n",
    "\n",
    "# Main exercise (60 points)\n",
    "\n",
    "Implement the Viterbi algorithm fot $k=3$.\n",
    "\n",
    "The input can be found [here](http://sandbox.hlt.bme.hu/~gaebor/ea_anyag/python_nlp/).\n",
    "You can use the 1, 10 or 100 million word corpus, it is advised to use the 1 million corpus while you are developing and you can try te larger ones afterwards.\n",
    "\n",
    "Write a class called `Viterbi` with the following attributes:\n",
    "* `__init__`: has no arguments, except `self`\n",
    "* `train`: one argument (aside `self`), an iterable object which generates 2-tuples of strings `[(word1, pos1), (word2, pos2), ...]`\n",
    "  * initializes the vocabularies, empirical probabilities or any other data attributes needed for the algorithm.\n",
    "  * you can read the data (generator) only once\n",
    "  * returns `None`\n",
    "* predict: one argument (aside `self`), an iterable object, a list of words.\n",
    "  * returns the predicted label sequence: a list of labels with the same length as the input.\n",
    "\n",
    "Don't use global variables!\n",
    "\n",
    "### Hint\n",
    "* Use a 3-dimensional array for the transition probabilities $\\mathbb{P}(v \\ | \\ w, u)$.\n",
    "* Use a 3-dimensional array for the Viterbi table, one index for time, one index for the previous state and one index for the state before that.\n",
    "* Same for the backtracking table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Viterbi(object):\n",
    "    pass\n",
    "\n",
    "with open(\"umbc.casesensitive.word_pos.1M.txt\", \"r\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "    generator1 = (line.strip().split(\"\\t\") for line in f)\n",
    "    generator2 = (line for line in generator1 if len(line) == 2)\n",
    "\n",
    "    viterbi = Viterbi()\n",
    "    viterbi.train(generator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(viterbi.predict(\"You talk the talk .\".split()) == ['PRP', 'VB', 'DT', 'NN', '.'])\n",
    "print(viterbi.predict(\"The dog .\".split()))\n",
    "print(viterbi.predict(\"The dog runs .\".split()))\n",
    "print(viterbi.predict(\"The dog runs slowly .\".split()))\n",
    "print(viterbi.predict(\"The dog 's run was slow .\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small exercise 1. (10 points)\n",
    "Modify the Viterbi class: use a logarithmic scale for probabilities.\n",
    "\n",
    "In the Viterbi table instead of \n",
    "$$\n",
    "\\pi(k,u,v) = \\max_{w\\in L} \\pi(k-1, w, u)\\cdot \\mathbb{P}(v \\ | \\ w,u)\\cdot \\mathbb{P}(w_k \\ | \\ v) \n",
    "$$\n",
    "use\n",
    "$$\n",
    "\\hat\\pi(k,u,v) = \\max_{w\\in L} \\hat\\pi(kâˆ’1,w,u) + \\log\\mathbb{P}(v \\ | \\ w,u) + \\log\\mathbb{P}(w_k \\ | \\ v) \n",
    "$$\n",
    "\n",
    "Note that the minimum probability is $0$, but the minimum logarithm is $-\\infty$. Both numpy and python float can deal with minus infinity.<br>\n",
    "Precalculate the log-probabilities in the initializer, not during the dymanic programming.\n",
    "\n",
    "This should not affect the result, just the numbers in the viterbi table.\n",
    "\n",
    "Name the log-scaled imlementation `ViterbiLog`, it can inherit from `Viterbi` or it can be a whole new class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"umbc.casesensitive.word_pos.1M.txt\", \"r\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "    generator1 = (line.strip().split(\"\\t\") for line in f)\n",
    "    generator2 = (line for line in generator1 if len(line) == 2)\n",
    "\n",
    "    viterbi_log = ViterbiLog()\n",
    "    viterbi_log.train(generator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(viterbi.predict(\"The dog runs slowly .\".split()) == viterbi_log.predict(\"The dog runs slowly .\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small exercise 2. (30 points)\n",
    "### a) 15 points\n",
    "Modify the Viterbi class: use a sparse storage for transition probabilities, not a 3-dimensional array.\n",
    "\n",
    "Use a dict to store the frequencies of the 2 and 3 tuples of labels.\n",
    "\n",
    "For example if you had _\"adjective noun\"_ 10 times and _\"adjective noun determinant\"_ 5 times, then store the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{('JJ', 'NN'): 10, ('JJ', 'NN', 'DT'): 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example $\\mathbb{P}(DT \\ | \\ JJ, NN ) = 0.5$\n",
    "\n",
    "Note that whenever $\\#\\{JJ, NN\\} = 0$ or $\\#\\{JJ, NN, DT\\} = 0$, then $\\mathbb{P}(DT \\ | \\ JJ, NN ) = 0$.\n",
    "\n",
    "Implement this in a new class `ViterbiSparse`, it can inherit from the original one or it can be a new class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) 15 points\n",
    "Try to find a sparse representation (with `dict`-s) which makes the inner for loop shorter. Note that you don't have to take the maximum over all the $w\\in L$ elements, if you already know that some transition probabilities are zeros.\n",
    "\n",
    "$$\n",
    "\\max_{\\substack{w\\in L \\\\ \\mathbb{P}(v \\ | \\ w,u) > 0}} \\pi(k-1, w, u)\\cdot \\mathbb{P}(v \\ | \\ w,u)\\cdot \\mathbb{P}(w_k \\ | \\ v)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

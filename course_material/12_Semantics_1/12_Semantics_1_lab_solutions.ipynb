{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Semantics 1: words - Lab excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.E1 [Accessing WordNet using NLTK](#11.E1)\n",
    "\n",
    "### 11.E2 [Using word embeddings](#11.E2)\n",
    "\n",
    "### 11.E3 [Comparing WordNet and word embeddings](#11.E3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.E1 Accessing WordNet using NLTK\n",
    "<a id='11.E1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK (_Natural Language Toolkit_) is a python library for accessing many NLP tools and resources. The NLTK WordNet interface is described here: http://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK python package can be installed using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/recski/.virtualenvs/nlp_course/lib/python3.5/site-packages\r\n",
      "Requirement already satisfied: six in /home/recski/.virtualenvs/nlp_course/lib/python3.5/site-packages (from nltk)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import nltk and use its internal download tool to get WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/recski/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the wordnet module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access synsets of a word using the _synsets_ function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('baseball_club.n.01'), Synset('club.n.02'), Synset('club.n.03'), Synset('clubhouse.n.01'), Synset('golf_club.n.02'), Synset('club.n.06'), Synset('cabaret.n.01'), Synset('club.v.01'), Synset('club.v.02'), Synset('club.v.03'), Synset('club.v.04')]\n"
     ]
    }
   ],
   "source": [
    "club_synsets = wn.synsets('club')\n",
    "print(club_synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each synset has a _definition_ function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseball_club.n.01\ta team of professional baseball players who play and travel together\n",
      "club.n.02\ta formal association of people with similar interests\n",
      "club.n.03\tstout stick that is larger at one end\n",
      "clubhouse.n.01\ta building that is occupied by a social club\n",
      "golf_club.n.02\tgolf equipment used by a golfer to hit a golf ball\n",
      "club.n.06\ta playing card in the minor suit that has one or more black trefoils on it\n",
      "cabaret.n.01\ta spot that is open late at night and that provides entertainment (as singers or dancers) as well as dancing and food and drink\n",
      "club.v.01\tunite with a common purpose\n",
      "club.v.02\tgather and spend time together\n",
      "club.v.03\tstrike with a club or a bludgeon\n",
      "club.v.04\tgather into a club-like mass\n"
     ]
    }
   ],
   "source": [
    "for synset in club_synsets:\n",
    "    print(\"{0}\\t{1}\".format(synset.name(), synset.definition()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog = wn.synsets('dog')[0]\n",
    "dog.definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List lemmas of a synset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dog.n.01.dog'),\n",
       " Lemma('dog.n.01.domestic_dog'),\n",
       " Lemma('dog.n.01.Canis_familiaris')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.lemmas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List hypernyms and hyponyms of a synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('basenji.n.01'),\n",
       " Synset('corgi.n.01'),\n",
       " Synset('cur.n.01'),\n",
       " Synset('dalmatian.n.02'),\n",
       " Synset('great_pyrenees.n.01'),\n",
       " Synset('griffon.n.02'),\n",
       " Synset('hunting_dog.n.01'),\n",
       " Synset('lapdog.n.01'),\n",
       " Synset('leonberg.n.01'),\n",
       " Synset('mexican_hairless.n.01'),\n",
       " Synset('newfoundland.n.01'),\n",
       " Synset('pooch.n.01'),\n",
       " Synset('poodle.n.01'),\n",
       " Synset('pug.n.01'),\n",
       " Synset('puppy.n.01'),\n",
       " Synset('spitz.n.01'),\n",
       " Synset('toy_dog.n.01'),\n",
       " Synset('working_dog.n.01')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.hyponyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _closure_ method of synsets allows us to retrieve the transitive closure of the hypernym, hyponym, etc. relations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'),\n",
       " Synset('domestic_animal.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dog.closure(lambda s: s.hypernyms()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common_hypernyms and lowest_common_hypernyms work in relation to another synset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('carnivore.n.01')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = wn.synsets('cat')[0]\n",
    "dog.lowest_common_hypernyms(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('placental.n.01'),\n",
       " Synset('object.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('entity.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('physical_entity.n.01')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.common_hypernyms(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate through all synsets, possibly by POS-tag, use all_synsets, which returns a generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object WordNetCorpusReader.all_synsets at 0x7f7060ef3ca8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.all_synsets(pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity.n.01\n",
      "physical_entity.n.01\n",
      "abstraction.n.06\n",
      "thing.n.12\n",
      "object.n.01\n",
      "whole.n.02\n"
     ]
    }
   ],
   "source": [
    "for c, noun in enumerate(wn.all_synsets(pos='n')):\n",
    "    if c > 5:\n",
    "        break\n",
    "    print(noun.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Excercise (optional)__: use WordNet to implement the \"Guess the category\" game: the program lists lemmas that all share a hypernym, which the user has to guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.E2 Using word embeddings\n",
    "<a id='11.E2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download and extract the word embedding [glove.6B](http://nlp.stanford.edu/data/glove.6B.zip), which was trained on 6 billion words of English text using the [GloVe](https://nlp.stanford.edu/projects/glove/) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-11-30 15:11:51--  http://sandbox.hlt.bme.hu/~recski/stuff/glove.6B.50d.txt.gz\n",
      "Resolving sandbox.hlt.bme.hu (sandbox.hlt.bme.hu)... 152.66.88.21\n",
      "Connecting to sandbox.hlt.bme.hu (sandbox.hlt.bme.hu)|152.66.88.21|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 69182520 (66M) [application/x-gzip]\n",
      "Saving to: ‘glove.6B.50d.txt.gz’\n",
      "\n",
      "glove.6B.50d.txt.gz 100%[===================>]  65,98M  11,1MB/s    in 5,9s    \n",
      "\n",
      "2017-11-30 15:11:57 (11,1 MB/s) - ‘glove.6B.50d.txt.gz’ saved [69182520/69182520]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://sandbox.hlt.bme.hu/~recski/stuff/glove.6B.50d.txt.gz\n",
    "!gunzip -f glove.6B.50d.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the embedding into a 2D numpy array. Word forms should be stored in a separate 1D array. Also create a word index, a dictionary that returns the index of each word in the embedding. Vectors should be normalized to a length of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_embedding(fn):\n",
    "    words = []\n",
    "    emb = []\n",
    "    word_index = {}\n",
    "    c = 0\n",
    "    with open(fn, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split()\n",
    "            emb.append(np.array([float(i) for i in fields[1:]], dtype='float32'))\n",
    "            words.append(fields[0])\n",
    "            word_index[fields[0]] = c\n",
    "            c += 1\n",
    "\n",
    "    print(\"read {0} lines\".format(c))\n",
    "    return np.array(words), word_index, np.array(emb)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_embedding(emb):\n",
    "    return emb / np.linalg.norm(emb, axis=1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 400000 lines\n"
     ]
    }
   ],
   "source": [
    "words, word_index, emb = read_embedding('glove.6B.50d.txt')\n",
    "emb = normalize_embedding(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- write a function that takes two words and the embedding as input and returns their cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_sim(w1, w2, word_index, emb):\n",
    "    if w1 not in word_index or w2 not in word_index:\n",
    "        return None\n",
    "    return np.dot(emb[word_index[w1]], emb[word_index[w2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92180049"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_sim('cat', 'dog', word_index, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement a function that takes a word as a parameter and returns the 5 words that are closest to it in the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_n(word, words, word_index, emb, n=5):\n",
    "    try:\n",
    "        w_index = word_index[word]\n",
    "    except KeyError:\n",
    "        return None\n",
    "    w_vec = emb[w_index]\n",
    "\n",
    "    distances = np.dot(emb, w_vec)\n",
    "    indices = np.argsort(distances)[-n:][::-1]\n",
    "    return [words[i] for i in indices]          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'cat', 'dogs', 'horse', 'puppy']\n",
      "['king', 'prince', 'queen', 'ii', 'emperor']\n"
     ]
    }
   ],
   "source": [
    "print(nearest_n('dog', words, word_index, emb))\n",
    "print(nearest_n('king', words, word_index, emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.E3 Vector similarity in WordNet\n",
    "<a id='11.E3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code written in __11.E2__ to analyze word groups in WordNet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an embedding of WordNet synsets by mapping each of them to the mean of their lemmas' vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_synset(synset, words, word_index, emb):\n",
    "    word_set = [lemma.name() for lemma in synset.lemmas()]\n",
    "    indices = filter(None, map(word_index.get, word_set))\n",
    "    vecs = np.array([emb[i] for i in indices])\n",
    "    if len(vecs) == 0:\n",
    "        return None\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def embed_synsets(words, word_index, emb):\n",
    "    return {synset: embed_synset(synset, words, word_index, emb) for synset in wn.all_synsets()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_emb = embed_synsets(words, word_index, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- write a function that measures the similarity of two synsets based on the cosine similarity of their vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def synset_sim(ss1, ss2, synset_emb):\n",
    "    vec1 = synset_emb[ss1]\n",
    "    vec2 = synset_emb[ss2]\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return None\n",
    "    return np.dot(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92180049"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_sim(dog, cat, synset_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function that takes a synset as input and retrieves the n most similar synsets, using the above embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_n_synsets(synset, synset_emb, n=5):\n",
    "    distances = [(synset_sim(synset, other, synset_emb), other) for other in wn.all_synsets() if synset != other]\n",
    "    distances = [(sim, synset) for sim, synset in distances if not sim is None]\n",
    "    return sorted(distances, reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.43 s, sys: 76 ms, total: 3.51 s\n",
      "Wall time: 3.51 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.67804796, Synset('puffin.n.01')),\n",
       " (0.63854223, Synset('bantam.n.01')),\n",
       " (0.63685226, Synset('paperback_book.n.01')),\n",
       " (0.63685226, Synset('paperback.s.01')),\n",
       " (0.62880427, Synset('imprint.n.05')),\n",
       " (0.62880427, Synset('imprint.n.04')),\n",
       " (0.62880427, Synset('imprint.n.03')),\n",
       " (0.62880427, Synset('imprint.n.01')),\n",
       " (0.59954, Synset('signet.n.01')),\n",
       " (0.56959021, Synset('python.n.02'))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nearest_n_synsets(wn.synsets('penguin')[0], synset_emb, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build the list of all words that are both in wordnet and the GloVe embedding. On a sample of 100 such words, measure Spearman correlation of synset similarity and vector similarity (use scipy.stats.spearmanr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_in_both = [word for word in wn.all_lemma_names() if word in word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55666"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_in_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sample = random.sample(words_in_both, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compare_sims(sample, synset_emb, word_index, emb):\n",
    "    vec_sims, ss_sims = [], []\n",
    "    for w1 in sample:\n",
    "        for w2 in sample:\n",
    "            ss_sim = synset_sim(wn.synsets(w1)[0], wn.synsets(w2)[0], synset_emb)\n",
    "            if ss_sim is None:\n",
    "                continue\n",
    "            v_sim = vec_sim(w1, w2, word_index, emb)\n",
    "            vec_sims.append(v_sim)\n",
    "            ss_sims.append(ss_sim)\n",
    "    return spearmanr(vec_sims, ss_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.85134837070058833, pvalue=0.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_sims(sample, synset_emb, word_index, emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

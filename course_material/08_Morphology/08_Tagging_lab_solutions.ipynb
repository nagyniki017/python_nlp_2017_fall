{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Don't forget to download the data file [from here](http://sandbox.hlt.bme.hu/~gaebor/ea_anyag/python_nlp/)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "pos_dict = {}\n",
    "with open(\"umbc.casesensitive.word_pos.1M.txt\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        split_line = line.strip().split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word, pos = line.strip().split(\"\\t\")\n",
    "            if word not in word_dict:\n",
    "                index = len(word_dict)\n",
    "                word_dict[word] = index\n",
    "            if pos not in pos_dict:\n",
    "                index = len(pos_dict)\n",
    "                pos_dict[pos] = index\n",
    "index_to_word = {i: w for w, i in word_dict.items()}\n",
    "index_to_pos = {i: p for p, i in pos_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = numpy.zeros((len(word_dict), len(pos_dict)), dtype=int)\n",
    "N = numpy.zeros((len(pos_dict), len(pos_dict)), dtype=int)\n",
    "\n",
    "pos_list = []\n",
    "with open(\"umbc.casesensitive.word_pos.1M.txt\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        split_line = line.strip().split(\"\\t\")\n",
    "        if len(split_line) == 2:\n",
    "            word, pos = line.strip().split(\"\\t\")\n",
    "            pos_list.append(pos)\n",
    "            M[word_dict[word], pos_dict[pos]] += 1\n",
    "            \n",
    "            if len(pos_list) == 2:\n",
    "                N[pos_dict[pos_list[0]], pos_dict[pos_list[1]]] += 1\n",
    "                del pos_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = M / M.sum(axis=0)[None, :].astype(float)\n",
    "P2 = N / N.sum(axis=0)[None, :].astype(float)\n",
    "P2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi_step(previous, v, word):\n",
    "    best = 0.0\n",
    "    for w in pos_dict:\n",
    "        p = previous[pos_dict[w]] * P2[pos_dict[w], pos_dict[v]] * P1[word_dict[word], pos_dict[v]]\n",
    "        if p > best:\n",
    "            best = p\n",
    "    return best\n",
    "\n",
    "def viterbi_step_index(previous, v, word):\n",
    "    \"\"\"the same but with integer IDs, not strings\"\"\"\n",
    "    best = 0.0\n",
    "    for w in range(len(pos_dict)):\n",
    "        p = previous[w] * P2[w, v] * P1[word, v]\n",
    "        if p > best:\n",
    "            best = p\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(words):\n",
    "    pi = numpy.zeros((len(pos_dict), len(words)), dtype=float)\n",
    "    for v in pos_dict:\n",
    "        pi[pos_dict[v], 0] = P1[word_dict[words[0]], pos_dict[v]]\n",
    "    \n",
    "    for k in range(1, len(words)):\n",
    "        for v in pos_dict:\n",
    "            value = viterbi_step(pi[:, k-1], v, words[k])\n",
    "            pi[pos_dict[v], k] = value\n",
    "\n",
    "    best_ending_v = 0\n",
    "    for v in range(len(pos_dict)):\n",
    "            if pi[v, -1] > pi[best_ending_v, -1]:\n",
    "                best_ending_v = v\n",
    "    return pi[best_ending_v, -1]\n",
    "\n",
    "def viterbi_index(words):\n",
    "    \"\"\"the same but with integer IDs, not strings\"\"\"\n",
    "    pi = numpy.zeros((len(pos_dict), len(words)), dtype=float)\n",
    "    for v in range(len(pos_dict)):\n",
    "        pi[v, 0] = P1[word_dict[words[0]], v]\n",
    "    \n",
    "    for k in range(1, len(words)):\n",
    "        for v in range(len(pos_dict)):\n",
    "            value = viterbi_step_index(pi[:, k-1], v, word_dict[words[k]])\n",
    "            pi[v, k] = value\n",
    "\n",
    "    best_ending_v = 0\n",
    "    for v in range(len(pos_dict)):\n",
    "        if pi[v, -1] > pi[best_ending_v, -1]:\n",
    "            best_ending_v = v\n",
    "    return pi[best_ending_v, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi(\"The cat sat on the desk .\".split())\n",
    "viterbi_index(\"The cat sat on the desk .\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "Add the backtracking with an extra table, which stores the argmax, not the max value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi_step(previous, v, word):\n",
    "    \"\"\"\n",
    "    using indices\n",
    "    returning max value and argmax\n",
    "    \"\"\"\n",
    "    \n",
    "    best = 0.0\n",
    "    best_choice = 0\n",
    "    for w in range(len(pos_dict)):\n",
    "        p = previous[w] * P2[w, v] * P1[word, v]\n",
    "        if p > best:\n",
    "            best = p\n",
    "            best_choice = w\n",
    "    return best, best_choice\n",
    "\n",
    "def viterbi(words):\n",
    "    pi = numpy.zeros((len(pos_dict), len(words)), dtype=float)\n",
    "    choices = numpy.zeros((len(pos_dict), len(words)), dtype=\"uint32\")\n",
    "    \n",
    "    for v in range(len(pos_dict)):\n",
    "        pi[v, 0] = P1[word_dict[words[0]], v]\n",
    "    \n",
    "    for k in range(1, len(words)):\n",
    "        for v in range(len(pos_dict)):\n",
    "            value, choice = viterbi_step(pi[:, k-1], v, word_dict[words[k]])\n",
    "            pi[v, k] = value\n",
    "            choices[v, k] = choice\n",
    "\n",
    "    best_ending_v = 0\n",
    "    for v in range(len(pos_dict)):\n",
    "        if pi[v, -1] > pi[best_ending_v, -1]:\n",
    "            best_ending_v = v\n",
    "    \n",
    "    optimal = [best_ending_v]\n",
    "    \n",
    "    # backtracking\n",
    "    for k in range(len(words)-1, 0, -1):\n",
    "        # prepend optimal sequence\n",
    "        optimal = [choices[optimal[0], k]] + optimal\n",
    "        \n",
    "    return pi[best_ending_v, -1], [index_to_pos[p] for p in optimal]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi(\"The dog sat on the cat .\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

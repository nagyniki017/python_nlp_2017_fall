{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging lab\n",
    "## Viterbi\n",
    "$k = 2$\n",
    "The Markov assumption:\n",
    "$$\n",
    "\\mathbb{P}(w_1, w_2, \\ldots w_n \\ | \\ l_1, l_2 \\ldots l_n) =\n",
    "    \\prod_{i=1}^n\\mathbb{P}(l_i \\ | l_{i-1})\\cdot\\mathbb{P}(w_i \\ | \\ l_i)\n",
    "$$\n",
    "The Viterbi function gives the optimal value:\n",
    "$$\n",
    "V(w_1, w_2, \\ldots w_n) =\n",
    "{\\arg\\max}_{l_i\\in L}\n",
    "    \\prod_{i=1}^n\\mathbb{P}(l_i \\ | l_{i-1})\\cdot\\mathbb{P}(w_i \\ | \\ l_i)\n",
    "$$\n",
    "For small $n$-s:\n",
    "$$\n",
    "V(w_1) = {\\arg\\max}_{l_1\\in L} \\mathbb{P}(w_1 \\ | \\ l_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "V(w_1, w_2) = {\\arg\\max}_{l_1, l_2\\in L} \\mathbb{P}(w_1 \\ | \\ l_1) \\cdot \\mathbb{P}(l_2 \\ | \\ l_1) \\cdot \\mathbb{P}(w_2 \\ | \\ l_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "V(w_1, w_2, w_3) = {\\arg\\max}_{l_1, l_2, l_3\\in L} V(w_1, w_2) \\cdot \\mathbb{P}(l_3 \\ | \\ l_2) \\cdot \\mathbb{P}(w_3 \\ | \\ l_3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "V(w_1, w_2, w_3, w_4) = {\\arg\\max}_{l_1, l_2, l_3, l_4\\in L} V(w_1, w_2, w_3) \\cdot \\mathbb{P}(l_4 \\ | \\ l_3) \\cdot \\mathbb{P}(w_4 \\ | \\ l_4)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Viterbi\n",
    "For $k=2$\n",
    "### Task 1\n",
    "Read the file [`umbc.casesensitive.word_pos.1M.txt`](http://sandbox.hlt.bme.hu/~gaebor/ea_anyag/python_nlp) line-by-line and make a vocabulary of words and labels. The file is in a format:\n",
    "\n",
    "`\"word\\tpos\\n\"`\n",
    "You have to have:\n",
    "* a dict of words to indices\n",
    "* a dict of labels to indices\n",
    "* reverse dict, indices to words\n",
    "\n",
    "Note that there can be some errors in the file, character encoding and delimiter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "emissions = defaultdict(lambda: defaultdict(int))\n",
    "transitions = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "word_idx = defaultdict(lambda: len(word_idx))\n",
    "pos_idx = defaultdict(lambda: len(pos_idx))\n",
    "\n",
    "with open(\"umbc.casesensitive.word_pos.1M.txt\", encoding=\"utf8\") as f:\n",
    "    first = next(f)\n",
    "    word, pos = first.rstrip('\\t').split('\\t')\n",
    "    word_idx[word]\n",
    "    pos_idx[pos]\n",
    "    emissions[pos][word] += 1\n",
    "    for line in f:\n",
    "        fd = line.rstrip('\\n').split('\\t')\n",
    "        if len(fd) != 2:\n",
    "            continue\n",
    "        word = fd[0]\n",
    "        new_pos = fd[1]\n",
    "        emissions[new_pos][word] += 1\n",
    "        transitions[pos][new_pos] += 1\n",
    "        pos = new_pos\n",
    "        word_idx[word]\n",
    "        pos_idx[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Obtain the following matrices:\n",
    "* counts of words with pos tags\n",
    "  * a $|V|\\times |L|$ matrix of integers\n",
    "  $$M(i,j) = \\# \\ i^\\text{th} \\text{ word with pos tag } j$$\n",
    "  * an $|L|\\times |L|$ matrix of integers\n",
    "  $$N(i,j) = \\# \\ i^\\text{th} \\text{ pos after } j^\\text{th} \\text { pos}$$\n",
    "\n",
    "After that\n",
    "* empirical probabilities\n",
    "  * a $|V|\\times |L|$ matrix of floats\n",
    "  $$P_1(i,j) = \\frac{\\# \\ i^\\text{th} \\text{ word with pos tag } j}{\\# \\ \\text{pos tag } j}$$\n",
    "  * an $|L|\\times |L|$ matrix of floats\n",
    "  $$P_2(i,j) = \\frac{\\# \\ i^\\text{th} \\text{ pos after } j^\\text{th} \\text { pos}}{\\# \\ \\text{pos tag } j}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "em_probs = np.zeros((len(pos_idx), len(word_idx)), dtype=\"float32\")\n",
    "\n",
    "for pos, em_freqs in emissions.items():\n",
    "    for word, cnt in em_freqs.items():\n",
    "        em_probs[pos_idx[pos], word_idx[word]] = cnt\n",
    "em_probs = em_probs / em_probs.sum(axis=1).reshape(em_probs.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_probs = np.zeros((len(pos_idx), len(pos_idx)), dtype=\"float32\")\n",
    "\n",
    "for pos1, tr in transitions.items():\n",
    "    for pos2, cnt in tr.items():\n",
    "        tr_probs[pos_idx[pos1], pos_idx[pos2]] = cnt\n",
    "tr_probs = tr_probs / tr_probs.sum(axis=1).reshape(tr_probs.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Implement the pseudocode in the tagging lecture ($k=2$).\n",
    "Use the global variables `P1` and `P2` (and the dictionaries).\n",
    "\n",
    "It is useful to have a one-step-viterbi function.\n",
    "$$\n",
    "\\mathrm{step}(\\pi_{k-1}, v, word) = \\max_{w\\in L} \\left(\\pi(kâˆ’1,w)\\cdot \\mathbb{P}(v \\ | \\ w)\\cdot \\mathbb{P}(word \\ | \\ v)\\right) \n",
    "$$\n",
    "$\\pi_{k-1}$ is a vector, the previous column of the dynamic table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P1 = em_probs\n",
    "P2 = tr_probs\n",
    "\n",
    "idx_word = {v: k for k, v in word_idx.items()}\n",
    "idx_pos = {v: k for k, v in pos_idx.items()}\n",
    "\n",
    "def viterbi_step(previous, v, w, word):\n",
    "    return previous[pos_idx[w]] * P2[pos_idx[w], pos_idx[v]] * P1[pos_idx[v], word_idx[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['DT', 'NN', 'NNS', '.', 'DT', 'NN', 'NNS', '.', 'DT', 'NN'],\n",
       " ['NNS', '.', 'DT', 'NN', 'NNS', '.', 'DT', 'NN', 'NNS', '.'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def viterbi(words):\n",
    "    \"\"\"from a list of strings returns the optimal probability\"\"\"\n",
    "    V = np.zeros((len(pos_idx), len(words)), dtype=\"float64\")\n",
    "    backptr = np.zeros((len(pos_idx), len(words)), dtype=\"int8\")\n",
    "    w0 = word_idx[words[0]]\n",
    "    V[:, 0] = P1[:, w0]\n",
    "    backptr[:, 0] = 0\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        word = words[i]\n",
    "        for pos in pos_idx.keys():\n",
    "            m = max((viterbi_step(V[:, i-1], pos, prev_pos, words[i]), prev_pos) for prev_pos in pos_idx.keys())\n",
    "            V[pos_idx[pos], i] = m[0]\n",
    "            backptr[pos_idx[pos], i] = pos_idx[m[1]]\n",
    "    decoded = [idx_pos[np.argmax(V[:, -1])]]\n",
    "    for i in range(len(words)-1, -1, -1):\n",
    "        bp = backptr[pos_idx[decoded[-1]], i]\n",
    "        decoded.append(idx_pos[bp])\n",
    "    return decoded[::-1][1:]\n",
    "\n",
    "s = \"the dog runs .\"\n",
    "words = s.split() * 5\n",
    "tags = viterbi(words)\n",
    "print(len(tags))\n",
    "tags[:10], tags[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "Add the backtracking with an extra table, which stores the argmax, not the max value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy, scipy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"umbc.casesensitive.word_pos.1M.txt\", \"rb\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(b'\\t')\n",
    "        if len(parts) == 2:\n",
    "            word = parts[0]\n",
    "            pos = parts[1]\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory 02\n",
    "\n",
    "## Requirements\n",
    "\n",
    "For the second part of the exercises you will need the `wikipedia` package. On Windows machines, use the following command in the Anaconda Prompt (`Start --> Anaconda --> Anaconda Prompt`):\n",
    "\n",
    "    conda install -c conda-forge wikipedia\n",
    "    \n",
    "This command should work with other Anaconda environments (OSX, Linux).\n",
    "\n",
    "If you are using virtualenv directly instead of Anaconda, the following command installs it in your virtualenv:\n",
    "\n",
    "    pip install wikipedia\n",
    "\n",
    "or\n",
    "\n",
    "    sudo pip install wikipedia\n",
    "    \n",
    "installs it system-wide.\n",
    "\n",
    "You are encouraged to reuse functions that you defined in earlier exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Define a function that takes a sequence as its input and returns whether the sequence is symmetric. A sequence is symmetric if it is equal to its reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_symmetric(l):\n",
    "    for x, y in zip(l[::1], l[::-1]):\n",
    "        if x != y:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "assert(is_symmetric([1]) == True)\n",
    "assert(is_symmetric([]) == True)\n",
    "assert(is_symmetric([1, 2, 3, 1]) == False)\n",
    "assert(is_symmetric([1, \"foo\", \"bar\", \"foo\", 1]) == True)\n",
    "assert(is_symmetric(\"abcba\") == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Define a function that takes a sequence and an integer $k$ as its input and returns the $k$ largest element. Do not use the built-in `max` function. Do not change the original sequence. If $k$ is not specified return one element in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_largest(l, k=1, key=None):\n",
    "    if key is None:\n",
    "        return sorted(l, reverse=True)[:k]\n",
    "    else:\n",
    "        return sorted(l, reverse=True, key= lambda x: x[key])[:k]\n",
    "\n",
    "l = [-1, 0, 3, 2]\n",
    "\n",
    "assert(k_largest(l) == [3])\n",
    "assert(k_largest(l, 2) == [2, 3] or k_largest(l, 2))\n",
    "assert(k_largest([{'a': 1}, {'a': 2}], key='a') == [{'a': 2}] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\*1.3 Add an optional `key` argument that works analogously to the built-in `sorted`'s key argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that takes a matrix as an input represented as a list of lists (you can assume that the input is a valid matrix). Return its transpose without changing the original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transpose(M):\n",
    "    return np.array(M).T.tolist()\n",
    "\n",
    "m1 = [[1, 2, 3], [4, 5, 6]]\n",
    "m2 = [[1, 4], [2, 5], [3, 6]]\n",
    "\n",
    "assert(transpose(m1) == m2)\n",
    "assert(transpose(transpose(m1)) == m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define a function that takes a string as its input and return a dictionary with the character frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_freq(s):\n",
    "    freq = {}\n",
    "    for char in s:\n",
    "        if char in freq:\n",
    "            freq[char] += 1\n",
    "        else:\n",
    "            freq[char] = 1\n",
    "    return freq\n",
    "    \n",
    "assert(char_freq(\"aba\") == {\"a\": 2, \"b\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Add an optional `skip_symbols` to the `char_freq` function. `skip_symbols` is the set of symbols that should be excluded from the frequence dictionary. If this argument is not specified, the function should include every symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_freq_with_skip(s, skip_symbols=None):\n",
    "    freq = {}\n",
    "    for char in s:\n",
    "        if char not in skip_symbols:\n",
    "            if char in freq:\n",
    "                freq[char] += 1\n",
    "            else:\n",
    "                freq[char] = 1\n",
    "    return freq\n",
    "    \n",
    "assert(char_freq_with_skip(\"ab.abc?\", skip_symbols=\".?\") == {\"a\": 2, \"b\": 2, \"c\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define a function that computes word frequencies in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(s):\n",
    "    words = s.split(' ')\n",
    "    freq = {}\n",
    "    for word in words:\n",
    "        if word in freq:\n",
    "            freq[word] += 1\n",
    "        else:\n",
    "            freq[word] = 1\n",
    "    return freq\n",
    "    \n",
    "    \n",
    "s = \"the green tea and the black tea\"\n",
    "\n",
    "assert(word_freq(s) == {\"the\": 2, \"tea\": 2, \"green\": 1, \"black\": 1, \"and\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Define a function that count the uppercase letters in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_upper_case(s):\n",
    "    upper_count = 0\n",
    "    for char in s:\n",
    "        if char.isupper():\n",
    "            upper_count +=1\n",
    "    return upper_count\n",
    "    \n",
    "assert(count_upper_case(\"A\") == 1)\n",
    "assert(count_upper_case(\"abA bcCa\") == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Define a function that takes two strings and decides whether they are anagrams. A string is an anagram of another string if its letters can be rearranged so that it equals the other string.\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "abc -- bac\n",
    "aabb -- abab\n",
    "```\n",
    "\n",
    "Counter examples:\n",
    "\n",
    "```\n",
    "abc -- aabc\n",
    "abab -- aaab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def anagram(s1, s2):\n",
    "    return sorted(s1) == sorted(s2)\n",
    "\n",
    "assert(anagram(\"abc\", \"bac\") == True)\n",
    "assert(anagram(\"aabb\", \"abab\") == True)\n",
    "assert(anagram(\"abab\", \"aaab\") == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Define a sentence splitter function that takes a string and splits it into a list of sentences. Sentences end with `.` and the new sentence must start with a whitespace (`str.isspace`) or be the end of the string. See the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'b', ' acd', '']\n",
      "['A', ' b', ' acd', '']\n"
     ]
    }
   ],
   "source": [
    "def sentence_splitter(s):\n",
    "    dot_splitted = s.split('.')\n",
    "    result = []\n",
    "    string = ''\n",
    "    print(dot_splitted)\n",
    "    for i in range(len(dot_splitted)-1):\n",
    "        if dot_splitted[i][0].isspace():\n",
    "            result.append(string)\n",
    "            string = dot_splitted[i].strip()\n",
    "        else:\n",
    "            if len(string) > 0:\n",
    "                string += '.'\n",
    "            string += dot_splitted[i]\n",
    "    result.append(string)\n",
    "    return result\n",
    "            \n",
    "        \n",
    "assert(sentence_splitter(\"A.b. acd.\") == ['A.b', 'acd'])\n",
    "assert(sentence_splitter(\"A. b. acd.\") == ['A', 'b', 'acd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia module\n",
    "\n",
    "The following exercises use the `wikipedia` package. The basic usage is illustrated below.\n",
    "\n",
    "The documentation is available [here](https://pypi.python.org/pypi/wikipedia/).\n",
    "\n",
    "Searching for pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Budapest',\n",
       " 'The Grand Budapest Hotel',\n",
       " 'Vilmos Kondor',\n",
       " 'Siege of Budapest',\n",
       " 'Budapest (song)',\n",
       " 'List of films shot in Budapest',\n",
       " 'Hungarian Parliament Building',\n",
       " 'Budapest Metro',\n",
       " 'Budapest Ferenc Liszt International Airport',\n",
       " 'Budapest Highflyer']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "results = wikipedia.search(\"Budapest\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading an article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Budapest (Hungarian: [ˈbudɒpɛʃt]) is the capital and by far the most populous city of Hungary and on'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = wikipedia.page(\"Budapest\")\n",
    "\n",
    "article.summary[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content attribute contains the full text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, 83831)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(article.content), len(article.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the module downloads the English Wikipedia. The language can be changed the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikipedia.set_lang(\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Budapest',\n",
       " 'Budapest Honvéd',\n",
       " 'Gare de Budapest-Nyugati',\n",
       " 'Arrondissements de Budapest',\n",
       " 'Métro de Budapest',\n",
       " 'Bataille de Budapest',\n",
       " 'Papp László Budapest Sportaréna',\n",
       " 'MTK Budapest FC',\n",
       " 'Gare de Budapest-Déli',\n",
       " 'MTK Budapest']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.search(\"Budapest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Budapest (prononcé [by.da.ˈpɛst] , hongrois : Budapest [ˈbu.dɒ.pɛʃt]  ; allemand : Budapest ou ancie'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_article = wikipedia.page(\"Budapest\")\n",
    "fr_article.summary[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Change the language back to English and test the package with a few other pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikipedia.set_lang(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Download 4-5 arbitrary pages from the English Wikipedia (they should exceed 100000 characters combined) and compute the word frequencies using your previously defined function(s). Print the most common 20 words in the following format (the example is not the correct answer):\n",
    "\n",
    "```\n",
    "unintelligent <TAB>  123456\n",
    "moribund <TAB>   123451\n",
    "...\n",
    "```\n",
    "\n",
    "The words and their frequency are separated by TABS and no additional whitespace should be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 5759)\n",
      "('of', 3196)\n",
      "('and', 2766)\n",
      "('in', 2327)\n",
      "('to', 1113)\n",
      "('a', 1018)\n",
      "('is', 1009)\n",
      "('The', 743)\n",
      "('by', 530)\n",
      "('as', 525)\n",
      "('was', 478)\n",
      "('for', 475)\n",
      "('are', 453)\n",
      "('with', 449)\n",
      "('New', 385)\n",
      "('from', 335)\n",
      "('has', 329)\n",
      "('York', 323)\n",
      "('London', 307)\n",
      "('on', 297)\n",
      "482812\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def load_articles(pages):\n",
    "    articles = []\n",
    "    for page in pages:\n",
    "        articles.append(wikipedia.page(page))\n",
    "    return articles\n",
    "\n",
    "def get_content_of_articles(articles):\n",
    "    content = ''\n",
    "    for article in articles:\n",
    "        content += article.content + '\\n'\n",
    "    return content\n",
    "\n",
    "def n_most_frequent(d, n=1, rev=True):\n",
    "    most_freq = sorted(d.items(), reverse=rev, key=operator.itemgetter(1))[:n]\n",
    "    for element in most_freq:\n",
    "        print(element)\n",
    "\n",
    "def word_freq_of_pages(pages):\n",
    "    freqs = {}\n",
    "\n",
    "    for page in pages:\n",
    "        article = wikipedia.page(page)\n",
    "        article_freq = word_freq(article.content)\n",
    "        for word in article_freq.keys():\n",
    "            if word in freqs:\n",
    "                freqs[word] += article_freq[word]\n",
    "            else:\n",
    "                freqs[word] = article_freq[word]\n",
    "\n",
    "\n",
    "    most_freq = sorted(freqs.items(), reverse=True, key=operator.itemgetter(1))[:20]\n",
    "    for element in most_freq:\n",
    "        print(element)\n",
    "    \n",
    "wikipedia.set_lang(\"en\")\n",
    "pages = [\"Hungary\", \"Budapest\", \"New York City\", \"United Kingdom\", \"London\"]\n",
    "english_articles = load_articles(pages)\n",
    "english_content = get_content_of_articles(english_articles)\n",
    "n_most_frequent(word_freq(english_content), 20)\n",
    "print(len(english_content))\n",
    "\n",
    "# word_freq_of_pages(pages)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Repeat the same exercise for your native language if it denotes word boundaries with spaces. If it doesn't choose an arbitrary language other than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 2694)\n",
      "('és', 920)\n",
      "('az', 816)\n",
      "('A', 535)\n",
      "('is', 255)\n",
      "('Az', 179)\n",
      "('város', 113)\n",
      "('New', 103)\n",
      "('ország', 99)\n",
      "('mint', 99)\n",
      "('legnagyobb', 94)\n",
      "('magyar', 91)\n",
      "('egy', 91)\n",
      "('több', 87)\n",
      "('nem', 82)\n",
      "('–', 79)\n",
      "('vagy', 78)\n",
      "('York', 76)\n",
      "('London', 73)\n",
      "('Magyarország', 71)\n",
      "246592\n"
     ]
    }
   ],
   "source": [
    "wikipedia.set_lang(\"hu\")\n",
    "pages = [\"Magyarország\", \"Budapest\", \"New York\", \"Egyesült Királyság\", \"London\"]\n",
    "hungarian_articles = load_articles(pages)\n",
    "hungarian_content = get_content_of_articles(hungarian_articles)\n",
    "n_most_frequent(word_freq(hungarian_content), 20)\n",
    "print(len(hungarian_content))\n",
    "\n",
    "# word_freq_of_pages(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Define a function that takes a string and returns its bigram frequencies as a dictionary.\n",
    "\n",
    "Character bigrams are pairs of subsequent characters. For example word `apple` contains the following bigrams: `ap, pp, pl, le`.\n",
    "\n",
    "They are used for language modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ap': 2, 'pp': 2, 'pl': 2, 'le': 2}\n"
     ]
    }
   ],
   "source": [
    "def bigram_freq(s, bi_freq=None):\n",
    "    if bi_freq is None:\n",
    "        bi_freq = {}\n",
    "    for i in range(len(s)-1):\n",
    "        sub = s[i:i+2]\n",
    "        if sub in bi_freq:\n",
    "            bi_freq[sub] += 1\n",
    "        else:\n",
    "            bi_freq[sub] = 1\n",
    "    return bi_freq\n",
    "    \n",
    "print(bigram_freq('apple', bigram_freq('apple')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Using your previous English collection compute bigram frequencies.\n",
    "\n",
    "What are the 10 most common and 10 least common bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most comon:\n",
      "('e ', 12734)\n",
      "(' t', 9166)\n",
      "('th', 8991)\n",
      "('he', 8425)\n",
      "('s ', 8262)\n",
      "('n ', 7533)\n",
      "(' a', 7413)\n",
      "('in', 7384)\n",
      "('an', 7180)\n",
      "('d ', 6956)\n",
      "least comon:\n",
      "('(;', 1)\n",
      "('ˈm', 1)\n",
      "('mɒ', 1)\n",
      "('ɒɟ', 1)\n",
      "('ɟɒ', 1)\n",
      "('ɒr', 1)\n",
      "('ːɡ', 1)\n",
      "('ɡ]', 1)\n",
      "('o–', 1)\n",
      "('H\"', 1)\n"
     ]
    }
   ],
   "source": [
    "english_bifreq = bigram_freq(english_content)\n",
    "print(\"most comon:\")\n",
    "n_most_frequent(english_bifreq, 10, True)\n",
    "print(\"least comon:\")\n",
    "n_most_frequent(english_bifreq, 10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\*3.5 Define a function that takes two parameters: a string and an integer N and returns the N-gram frequencies of the string. For $N=2$ the function works the same as in the previous example.\n",
    "\n",
    "Try the function for $N=1..5$. How many unique N-grams are in your collection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique 1 -grams:  34\n",
      "Count of unique 2 -grams:  619\n",
      "Count of unique 3 -grams:  4993\n",
      "Count of unique 4 -grams:  20776\n",
      "Count of unique 5 -grams:  52681\n"
     ]
    }
   ],
   "source": [
    "def ngram_freq(s, n, n_freq=None):\n",
    "    if n_freq is None:\n",
    "        n_freq = {}\n",
    "    for i in range(len(s)-n+1):\n",
    "        sub = s[i:i+n]\n",
    "        if sub in n_freq:\n",
    "            n_freq[sub] += 1\n",
    "        else:\n",
    "            n_freq[sub] = 1\n",
    "    return n_freq\n",
    "\n",
    "def unique_ngrams(s, n):\n",
    "    unique_count = 0\n",
    "    stats = ngram_freq(s, i)\n",
    "    for key, value in stats.items():\n",
    "        if value == 1:\n",
    "            unique_count += 1\n",
    "    return unique_count\n",
    "    \n",
    "for i in range(1, 6):\n",
    "    print(\"Count of unique\", i, \"-grams: \", unique_ngrams(english_content, i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Compute the same statistics for your native language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique 1 -grams:  16\n",
      "Count of unique 2 -grams:  462\n",
      "Count of unique 3 -grams:  5175\n",
      "Count of unique 4 -grams:  22580\n",
      "Count of unique 5 -grams:  50922\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    print(\"Count of unique\", i, \"-grams: \", unique_ngrams(hungarian_content, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['. It', '. Wi', '. Th', '. Hu', '. Ma', '. Hi', '. By', '. Fo', '. On', '. As', '. We', '. Ac', '. Fr', '. He', '. A ', '. La', '. Af', '. Bo', '. In', '. Un', '. Ap', '. Kö', '. Up', '. Ki', '. Ov', '. Si', '. Ab', '. Am', '. Or', '. Co', '. To', '. Ec', '. Au', '. Ro', '. De', '. Ká', '. Le', '. Sz', '. Du', '. Ne', '. Ru', '. 20', '. So', '. Na', '. Rá', '. Be', '. Al', '. Jó', '. Li', '. Av', '. Te', '. 19', '. Tr', '. Im', '. Ho', '. El', '. Se', '. Bu', '. 13', '. Fa', '. Mo', '. Ot', '. Lo', '. Po', '. 1,', '. Ou', '. En', '. Sm', '. St', '. 10', '. Je', '. Ca', '. No', '. Wh', '. Pr', '. An', '. Pl', '. 28', '. Ta', '. Bé', '. Is', '. 79', '. Ol', '. Ja', '. Zr', '. (S', '. Já', '. Di', '. Pe', '. Pa', '. Gu', '. Sp', '. Gr', '. Wo', '. Ea', '. Fl', '. At', '. Ce', '. \\nS', '. GD', '. Op', '. Fu', '. Re', '. 18', '. Et', '. Sw', '. Wa', '. Fi', '. Of', '. Sn', '. Su', '. 31', '. Va', '. Mi', '. Ar', '. Pu', '. Nu', '. 30', '. 39', '. BS', '. Ga', '. Go', '. Ri', '. Sa', '. Ad', '. Da', '. Sk', '. BK', '. 4 ', '. Cu', '. Do', '. an', '. Me', '. DK', '. IS', '. \\nB', '. Vi', '. \\nU', '. Ya', '. \\nM', '. \\nF', '. NY', '. Ti', '. as', '. Br', '. Sl', '. Sh', '. un', '. Mc', '. st', '. Sc', '. If', '. Qu', '. Ke', '. (T', '. Ni', '. Ex', '. mi', '. Es', '. ci', '. Ge', '. Ko', '. Uk', '. Ch', '. Dy', '. \"N', '. ov', '. 45', '. ec', '. Ve', '. Bl', '. Tw', '. WB', '. WN', '. Jo', '. HH', '. Mu', '. po', '. wh', '. 54', '. al', '. JF', '. ga', '. Ci', '. 3,', '. wa', '. Ly', '. GB', '. Fe', '. Ur', '. Cr', '. Ba', '. HM', '. Ed', '. Ai', '. Ag', '. Ra', '. Dr', '. wi', '. 4.', '. 5.', '. 15', '. 71', '. G.', '. A.', '. H.', '. R.', '. S.', '. K.', '. M.', '. Gi', '. Gl', '. Tu', '. UK', '. \"F', '. Ir', '. xi', '. (1', '.  L', '. Ep', '. Ei', '. Cl', '. Eu', '. Ox', '. Kn', '. Wr', '. Hy', '. Ha', '. p.', '. 88', '. OC', '. \\n\\n']\n",
      "['. It', '. Wi', '. Th', '. Hu', '. Ma', '. Hi', '. By', '. Fo', '. On', '. As', '. We', '. Ac', '. Fr', '. He', '. A ', '. La', '. Af', '. Bo', '. In', '. Un', '. Ap', '. Kö', '. Up', '. Ki', '. Ov', '. Si', '. Ab', '. Am', '. Or', '. Co', '. To', '. Ec', '. Au', '. Ro', '. De', '. Ká', '. Le', '. Sz', '. Du', '. Ne', '. Ru', '. 20', '. So', '. Na', '. Rá', '. Be', '. Al', '. Jó', '. Li', '. Av', '. Te', '. 19', '. Tr', '. Im', '. Ho', '. El', '. Se', '. Bu', '. 13', '. Fa', '. Mo', '. Ot', '. Lo', '. Po', '. 1,', '. Ou', '. En', '. Sm', '. St', '. 10', '. Je', '. Ca', '. No', '. Wh', '. Pr', '. An', '. Pl', '. 28', '. Ta', '. Bé', '. Is', '. 79', '. Ol', '. Ja', '. Zr', '. (S', '. Já', '. Di', '. Pe', '. Pa', '. Gu', '. Sp', '. Gr', '. Wo', '. Ea', '. Fl', '. At', '. Ce', '. \\nS', '. GD', '. Op', '. Fu', '. Re', '. 18', '. Et', '. Sw', '. Wa', '. Fi', '. Of', '. Sn', '. Su', '. 31', '. Va', '. Mi', '. Ar', '. Pu', '. Nu', '. 30', '. 39', '. BS', '. Ga', '. Go', '. Ri', '. Sa', '. Ad', '. Da', '. Sk', '. BK', '. 4 ', '. Cu', '. Do', '. an', '. Me', '. DK', '. IS', '. \\nB', '. Vi', '. \\nU', '. Ya', '. \\nM', '. \\nF', '. NY', '. Ti', '. as', '. Br', '. Sl', '. Sh', '. un', '. Mc', '. st', '. Sc', '. If', '. Qu', '. Ke', '. (T', '. Ni', '. Ex', '. mi', '. Es', '. ci', '. Ge', '. Ko', '. Uk', '. Ch', '. Dy', '. \"N', '. ov', '. 45', '. ec', '. Ve', '. Bl', '. Tw', '. WB', '. WN', '. Jo', '. HH', '. Mu', '. po', '. wh', '. 54', '. al', '. JF', '. ga', '. Ci', '. 3,', '. wa', '. Ly', '. GB', '. Fe', '. Ur', '. Cr', '. Ba', '. HM', '. Ed', '. Ai', '. Ag', '. Ra', '. Dr', '. wi', '. 4.', '. 5.', '. 15', '. 71', '. G.', '. A.', '. H.', '. R.', '. S.', '. K.', '. M.', '. Gi', '. Gl', '. Tu', '. UK', '. \"F', '. Ir', '. xi', '. (1', '.  L', '. Ep', '. Ei', '. Cl', '. Eu', '. Ox', '. Kn', '. Wr', '. Hy', '. Ha', '. p.', '. 88', '. OC', '. \\n\\n']\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "text = '. '\n",
    "N=4\n",
    "ngrams = ngram_freq(english_content, N)\n",
    "\n",
    "for i in range(2):\n",
    "    possible_next = []\n",
    "    ending = text[-N+1:]\n",
    "    if len(ending) < N:\n",
    "        char_num = len(ending)\n",
    "    else:\n",
    "        char_num = N\n",
    "    for ngram in ngrams:\n",
    "        if (ngram[:char_num] == ending):\n",
    "            possible_next.append(ngram)\n",
    "    print(possible_next)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
